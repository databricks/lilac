{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import lilac as ll\n",
    "\n",
    "ds = ll.get_dataset('local', 'hncomments-1m')\n",
    "\n",
    "ll.data.clustering.compute_titles(ds, 'text__cluster')\n",
    "import lilac as ll\n",
    "from lilac.batch_utils import flatten_path_iter\n",
    "from lilac.schema import VALUE_KEY, normalize_path\n",
    "\n",
    "namespace = 'local'\n",
    "dataset = 'capybara'\n",
    "ds = ll.get_dataset(namespace, dataset)\n",
    "input = 'conversation.*.input'\n",
    "input_path = normalize_path(input)\n",
    "text_column = '_text'\n",
    "cluster_column = '_cluster'\n",
    "ds.cluster(input_path, output_path=cluster_column, remote=True)\n",
    "\n",
    "\n",
    "def flatten_input(item) -> str:\n",
    "  texts = flatten_path_iter(item, input_path)\n",
    "  # Filter out Nones\n",
    "  texts = (t for t in texts if t)\n",
    "  # Deal with enriched items.\n",
    "  texts = (t[VALUE_KEY] if VALUE_KEY in t else t for t in texts)\n",
    "  return '\\n'.join(texts)\n",
    "\n",
    "\n",
    "ds.map(flatten_input, output_path=text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "groups: dict[int, list[tuple[str, float]]] = {}\n",
    "for row in ds.select_rows([text_column, cluster_column]):\n",
    "  cluster_id = row[cluster_column]['cluster_id']\n",
    "  membership_prob = row[cluster_column]['cluster_membership_prob']\n",
    "  groups.setdefault(cluster_id, []).append((row[text_column], membership_prob))\n",
    "\n",
    "# Sort by descending membership score.\n",
    "for cluster_id, group in groups.items():\n",
    "  # Remove any duplicate texts in the group.\n",
    "  group = list(set(group))\n",
    "  # Shuffle the group to avoid biasing the topic function.\n",
    "  random.shuffle(group)\n",
    "  group.sort(key=lambda text_score: text_score[1], reverse=True)\n",
    "  groups[cluster_id] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:00<00:00, 6296.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"capybara_clusters\" written to ./data/datasets/local/capybara_clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lilac.data.dataset_duckdb.DatasetDuckDB at 0x2b5916b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [\n",
    "  {'cluster_id': cluster_id, 'texts': [{'text': text, 'prob': prob} for text, prob in g]}\n",
    "  for cluster_id, g in groups.items()\n",
    "]\n",
    "ds = ll.from_dicts(namespace, f'{dataset}_clusters', items, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_id: int64\n",
      "texts: list(\n",
      "  text: string\n",
      "  prob: float64)\n"
     ]
    }
   ],
   "source": [
    "print(ds.manifest().data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/capybara_clusters][64 shards] map \"title\" to \"('title_v6',)\": 100%|██████████| 417/417 [00:11<00:00, 35.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to ./data/datasets/local/capybara_clusters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lilac.data.dataset_duckdb.DuckDBMapOutput at 0x288fb2a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lilac as ll\n",
    "from lilac.data.clustering import summarize_request\n",
    "\n",
    "namespace = 'local'\n",
    "dataset = 'capybara'\n",
    "ds = ll.get_dataset(namespace, f'{dataset}_clusters')\n",
    "\n",
    "\n",
    "def title(item):\n",
    "  ranked_docs = [(x['text'], x['prob']) for x in item['texts']]\n",
    "  title = summarize_request(ranked_docs)\n",
    "  return title\n",
    "\n",
    "\n",
    "ds.map(title, num_jobs=64, output_path='title_v6', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/capybara][1 shards] map \"extract_text\" to \"('clusters',)\": 100%|██████████| 16006/16006 [00:00<00:00, 27559.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to clusters-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/capybara][1 shards] map \"compute_clusters\" to \"('clusters',)\": 100%|██████████| 16006/16006 [00:51<00:00, 307.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to clusters-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/capybara][1 shards] map \"compute_cluster_titles\" to \"('clusters',)\": 100%|██████████| 16006/16006 [00:31<00:00, 511.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to clusters-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/capybara][1 shards] map \"compute_category_clusters\" to \"('clusters',)\": 100%|██████████| 16006/16006 [00:14<00:00, 1091.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to clusters-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/capybara][1 shards] map \"compute_category_titles\" to \"('clusters',)\": 100%|██████████| 16006/16006 [00:04<00:00, 3420.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to clusters-00000-of-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import lilac as ll\n",
    "\n",
    "ll.set_project_dir('./data')\n",
    "ds = ll.get_dataset('local', 'capybara')\n",
    "ll.data.clustering.cluster_impl(ds, 'conversation.*.input', output_path='clusters', remote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/hncomments-1m][1 shards] map \"extract_text\" to \"('text__cluster',)\": 100%|██████████| 1000000/1000000 [00:28<00:00, 34908.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to text__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/hncomments-1m][1 shards] map \"compute_cluster_titles\" to \"('text__cluster',)\": 100%|██████████| 1000000/1000000 [16:07<00:00, 1033.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to text__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/hncomments-1m][1 shards] map \"compute_category_clusters\" to \"('text__cluster',)\": 100%|██████████| 1000000/1000000 [01:05<00:00, 15177.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to text__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/hncomments-1m][1 shards] map \"compute_category_titles\" to \"('text__cluster',)\": 100%|██████████| 1000000/1000000 [00:31<00:00, 31462.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to text__cluster-00000-of-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import lilac as ll\n",
    "\n",
    "ds = ll.get_dataset('local', 'hncomments-1m')\n",
    "ll.data.clustering.cluster_impl(ds, 'text', remote=True, recompute_titles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dsmilkov/code/lilac/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[local/OpenHermes-2.5][1 shards] map \"extract_text\" to \"('prompt__cluster',)\": 100%|██████████| 1001551/1001551 [00:30<00:00, 32375.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to prompt__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/OpenHermes-2.5][1 shards] map \"compute_clusters\" to \"('prompt__cluster',)\": 100%|██████████| 1001551/1001551 [15:47<00:00, 1056.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to prompt__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/OpenHermes-2.5][1 shards] map \"compute_cluster_titles\" to \"('prompt__cluster',)\": 100%|██████████| 1001551/1001551 [13:18<00:00, 1254.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to prompt__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/OpenHermes-2.5][1 shards] map \"compute_category_clusters\" to \"('prompt__cluster',)\": 100%|██████████| 1001551/1001551 [01:17<00:00, 13002.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to prompt__cluster-00000-of-00001.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[local/OpenHermes-2.5][1 shards] map \"compute_category_titles\" to \"('prompt__cluster',)\": 100%|██████████| 1001551/1001551 [00:19<00:00, 51214.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote map output to prompt__cluster-00000-of-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "import lilac as ll\n",
    "\n",
    "ds = ll.get_dataset('local', 'OpenHermes-2.5')\n",
    "ll.data.clustering.cluster_impl(ds, 'prompt', remote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dsmilkov/code/lilac/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputing table+index for openorca... took 21.557s.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lilac as ll\n",
    "\n",
    "ll.set_project_dir('./data')\n",
    "ds = ll.get_dataset('local', 'openorca')\n",
    "# ds.cluster('question', remote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pivot took 0.008s.\n",
      "outer_groups=[OuterGroup(value='Answering Questions', count=1530, inner=[('Solving Linear Equations', 137), ('Answering Specific Questions from Text', 91), ('Extracting Details from Athlete Bios', 85), ('Translating, answering, and identifying questions', 74), ('Answering Location-Based Questions', 72), ('Answering Questions about the Mediterranean Diet', 65), ('NFL Game Questions', 65), ('Answering Questions About Traffic and Transportation', 64), ('Answering Text-Based Questions', 62), ('Answering Sports Trivia Questions', 61), ('Tech: Running Ubuntu from Windows', 59), ('Answering Various Questions', 58), ('Answering Comprehension Questions', 53), ('Answering Questions About Personal Information', 52), ('Answering Study and Test Questions', 52), ('Answering Specific Information Questions', 51), ('Answering Location Questions from Passages', 43), ('Answering Questions from Texts', 40), ('Answering Questions about Olympic Torch Relay', 40), ('Answering Conversation-Based Questions', 38), ('Answering questions about sports events', 38), ('Guessing Test Answers', 34), ('Performing Swapped Arithmetic Operations', 31), ('Complex Trivia Questions', 29), ('Answering Character Questions in Texts', 28), ('Answering Questions About Christian Denominations', 27), ('Guessing Identities from Personal Details', 23), ('Answering Common Sense Questions', 22), ('Validating Automatic System Answers', 18), ('Yard Conversions and Touchdowns', 18)]), OuterGroup(value='Sentence Manipulation', count=1276, inner=[('Analyzing Premises and Hypotheses', 167), ('Completing Sentence Prompts', 97), ('Sentence Contradiction Resolution', 95), ('Classifying research paper sentences', 79), (\"Understanding Narrator's Emotions After Earthquake\", 74), ('Describing Data in Sentences', 67), ('Paraphrasing e-commerce website development', 56), ('Ethical and Emotional Dilemma in Abortion Law', 50), ('Entailment and Paraphrasing Questions', 49), ('Paraphrasing and Sentence Meaning', 47), ('Correcting Punctuation in Sentences', 45), ('Analyzing Short Phrases with Participants', 45), ('Creating sentences with specific concepts', 41), ('Forming Sentences from Given Characters', 35), ('Interpreting Scenarios and Emotions', 35), ('Drawing Hypotheses from Context', 32), ('Completing Sentences in Gym Scenes', 31), ('Generating Plausible Vietnamese Sentences', 28), ('Paraphrasing and Sentence Equivalence Questions', 27), ('Analyzing True/False Sentences', 25), ('Sentence Similarity Scale', 24), ('Adding Punctuation to Sentences', 23), ('Creating sentences with specific words', 22), ('Capitalizing and Rewriting Instructions', 19), ('Correcting Punctuation and Case in Sentences', 18), ('Identifying Salient Words in Sentences', 17), ('Identifying Important Words in Sentences', 14), ('Interpreting Emotions and Actions in a Relationship', 14)]), OuterGroup(value='Summarization', count=1158, inner=[('Summarizing various articles', 79), ('Adding and Removing Spaces', 70), ('Matching and Summarizing Debate Arguments', 61), ('Methods for Extra Gooey Grilled Cheese', 60), (\"Detroit's Bankruptcy Plan Approval Date\", 59), ('Completing Short Stories', 54), ('Atomic Structure and Properties', 52), ('Subject Lines and Email Subjects', 51), ('Facebook News Feed ads', 48), (\"Arsenal's Clear-the-Air Talks After Drubbing\", 48), ('Summarize research paper abstract', 47), (\"Punctuating Samsung's 2018 Forecast\", 41), ('Identifying News Topics and Summaries', 41), ('Matching abstract to candidate titles', 38), ('Summarizing Short Dialogues', 36), ('Cleaning dirty light bulbs safely', 33), ('Reason for building new wing at palace', 32), ('Abuse and torture in Iraqi detention facilities', 32), ('Effects of Perturbations on Bird Life Cycle', 30), ('Summarizing News Articles', 30), ('Summarizing news articles and writing questions', 28), ('Subject Lines for Various Emails', 25), ('Dogs found shot in Searcy County', 24), ('Animal Parenting and Behavior', 24), (\"Birds' Egg Placement and Reproduction Study\", 23), ('Dealing with Side Cramps and Exercise', 21), ('Contact Lens Care and Safety', 21), ('Summarizing news articles', 21), ('Summarizing News Headlines', 17), (\"Iran's Nuclear Program and International Discord\", 12)]), OuterGroup(value='Translation', count=1115, inner=[('Translations and Language Identification', 111), ('Translating sentences between languages', 85), ('Identifying Languages in Texts', 79), ('Translations between Spanish and English', 68), ('German to English Translations', 57), ('Russian-English Translation and Language Identification', 54), ('Romanian Translations and Phrases', 54), ('Translate EU Agency Setup and Directives to Finnish', 52), ('Translating Romanian to English', 48), ('Translating Various Sentences to Turkish', 46), ('Translating Sentences Between Languages', 43), ('Russian Translations', 42), ('Translating Various Texts', 40), ('Translating English to Czech', 38), ('Translating sentences to French', 36), ('Translating Persian to English', 35), ('German Translations and Phrases', 30), ('Translate Tagalog and Filipino to Japanese and Thai', 26), ('C64 platformer flash version', 25), ('Choosing words in Indian languages', 23), ('Language mixture of Norfuk', 22), ('Translate Marathi to Oriya', 21), ('Linguistic Acceptability Evaluation', 20), ('Interpreting Religious Texts', 18), ('Language and Literature Tasks', 15), ('Translate English to Hindi', 14), ('Translating Macedonian News to Turkish', 13)]), OuterGroup(value='Review Analysis', count=1094, inner=[('Representing Restaurant Data', 223), ('Classifying sentiment of movie reviews', 206), ('Classifying sentiment of product reviews', 184), ('Analyzing Product Reviews', 79), ('Classifying sentiment of restaurant reviews', 71), ('Determining Product Recommendation Based on Review', 48), ('Predicting sentiment of tweets', 38), ('Generating movie reviews with sentiment', 37), ('Generating Positive and Negative Tweets', 36), ('Celebrity reactions to National Anthem', 28), ('App Recommendation Based on Reviews', 27), ('Rating of Short App Reviews', 26), ('Generating ratings for Amazon reviews', 21), ('Analyzing sentiment of restaurant reviews', 20), ('Generating App Reviews by Package', 19), ('Rating Reviews', 17), ('Generating Summary of Amazon Reviews', 14)]), OuterGroup(value='Content Creation', count=1076, inner=[('Interpreting Contextual Questions', 152), ('Answering specific questions from articles', 107), ('Analyzing Common-Sense Questions and Answers', 90), ('Answering Political Context Questions', 73), ('Creating Multi-Choice Questions from Articles', 71), ('Legal Aid Initiatives and Responses', 67), ('Answering Test Questions Based on Articles', 61), ('Extracting Answers from Contextual Questions', 55), ('Generating questions from answers', 46), ('Evaluating Question Suggestions', 38), ('Answering specific questions from various contexts', 36), ('Creating Questions About Glaciers and Snow', 36), (\"UKIP leader's stance on asylum policy\", 33), ('Creating Multi-Choice Questions', 30), ('Answering Multi-Choice Questions on Articles', 28), ('Answering Questions on Various Articles', 28), ('Proposed Rate Increases in California', 28), ('Answer Extraction and Commonsense Questions', 26), ('Creating multi-choice questions from articles', 23), ('Answering Context-Based Questions', 21), ('Validating Answers to Contextual Questions', 14), ('Extracting Answers from Context and Articles', 13)]), OuterGroup(value='Knowledge', count=876, inner=[('Biology Quiz Questions', 99), ('Historical Mileage and Telegraph Lines', 82), ('Extracting Historical Information', 74), ('Answering Science Questions', 64), ('Answering Archaeological Questions', 60), ('Art History Questions', 56), ('Answering Environmental Science Questions', 55), ('Questions about key figures in terrorism', 51), ('Validating Biology Questions and Answers', 51), ('Geological Periods and Taxonomy Questions', 47), ('Questions about African countries', 44), ('Historical Questions on World War I and II', 44), ('Details of Criminal Confession', 34), ('Identifying Individuals from Biographical Details', 23), ('Answering Science and History Questions', 22), ('Questions about ocean energy', 21), ('Answering science questions', 20), ('Historical Inquiries', 18), ('Ancient Timekeeping and Inventions', 11)]), OuterGroup(value='Movie Plot', count=805, inner=[('Answering Film Investment Questions', 140), ('Extracting Answers from Movie Plots', 110), ('Generating questions about movie plots', 89), ('Answering Specific Book and Movie Questions', 62), ('Answering Movie Plot Questions', 60), ('Questions about movie plots', 55), ('Questions about alien-themed movie plots', 52), ('Answering Questions About Various Films', 40), ('Movie Plot Questions and Answers', 40), ('Movie Plot Details', 37), ('Creating Movie Titles from Plots', 36), ('Creating Movie Plot Questions', 30), ('Inference Questions about Visual Scenarios', 29), ('Voice Actors in Animated Shows and Movies', 25)]), OuterGroup(value='Data Analysis', count=791, inner=[('Questions about music artists and albums', 131), ('Identifying Relations in Entertainment Industry', 87), ('Determining Music Facts', 70), ('Identifying Relations in Text', 70), ('Categorizing Various Titles', 60), ('Describing Music Genres and Artists', 59), ('Finding elements in a list', 49), ('Effects of African slave trade', 37), ('Classifying hateful Bengali posts', 30), ('Categorizing Political Texts', 28), ('Identifying Names in Music History', 27), ('Demographic Statistics Analysis', 25), ('Accusation of Sierra Leone attack', 25), ('Identifying POS Tags in Questions', 23), ('Comparing Viewership Demographics', 22), ('Word Categorization and Antonyms', 18), ('Filtering prime numbers from a list', 17), ('Identifying Health Risks in Different Age Groups', 13)]), OuterGroup(value='Science Education', count=279, inner=[('Understanding Gas Behavior in Physics', 58), ('Understanding Photons and Electromagnetic Force', 50), ('Understanding Energy Processes and Perturbations', 44), ('Understanding Oxygen Transport in the Body', 27), ('Understanding Process Perturbations', 21), ('Understanding Earthquake Concepts', 19), ('Understanding articles and questions', 17), ('Understanding Rock Formation Processes', 16), ('Gravity and Mass Questions', 15), ('Identifying Fossilization Process Steps', 12)])] too_many_distinct=False\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lilac as ll\n",
    "from lilac.utils import DebugTimer\n",
    "\n",
    "ll.set_project_dir('./data')\n",
    "ds = ll.get_dataset('local', 'SlimOrca-10k-sample')\n",
    "with DebugTimer('pivot'):\n",
    "  res = ds.pivot('prompt__cluster.category_title', 'prompt__cluster.cluster_title')\n",
    "print(res)\n",
    "\n",
    "ds = ll.get_dataset('local', 'openorca')\n",
    "with DebugTimer('pivot'):\n",
    "  res = ds.pivot('prompt__cluster.category_title', 'prompt__cluster.cluster_title')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────┬───────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
      "│       category       │                                       vals                                        │ cat_count │\n",
      "│       varchar        │                        struct(val varchar, count bigint)[]                        │  int128   │\n",
      "├──────────────────────┼───────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
      "│ Translation          │ [{'val': Translation Requests, 'count': 197479}, {'val': Translating English to…  │    298144 │\n",
      "│ Movie and Film Rev…  │ [{'val': Product and Movie Reviews, 'count': 144512}, {'val': Movie Review Sent…  │    185374 │\n",
      "│ Movie Questions      │ [{'val': Movie Plot Questions, 'count': 85310}, {'val': Answering Movie Plot Qu…  │    140575 │\n",
      "│ Inference Questions  │ [{'val': Reasoning and Inference Questions, 'count': 98845}, {'val': Logical In…  │    119380 │\n",
      "│ Sports Trivia        │ [{'val': Sports Trivia and Player Information, 'count': 90101}, {'val': NBA Pla…  │     92709 │\n",
      "│ Scenario Interpret…  │ [{'val': Interpreting Contextual Reactions, 'count': 82861}, {'val': Interpreti…  │     88424 │\n",
      "│ Restaurant Data      │ [{'val': Restaurant Reviews, 'count': 23887}, {'val': Creating Restaurant Descr…  │     78940 │\n",
      "│ Movie Plot Questions │ [{'val': Movie Plot Analysis, 'count': 14120}, {'val': Movie Plot Questions and…  │     74256 │\n",
      "│ Reading Comprehens…  │ [{'val': Answering Comprehension Questions, 'count': 6077}, {'val': Answering R…  │     66026 │\n",
      "│ Question Generation  │ [{'val': Movie Plot Creation, 'count': 29735}, {'val': Research Paper Analysis …  │     56966 │\n",
      "│        ·             │                                         ·                                         │        ·  │\n",
      "│        ·             │                                         ·                                         │        ·  │\n",
      "│        ·             │                                         ·                                         │        ·  │\n",
      "│ Player Positions     │ [{'val': Analyzing Gender Roles in Sports, 'count': 133}, {'val': NBA Player Po…  │       497 │\n",
      "│ Theatre and Perfor…  │ [{'val': Broadway Musical Theatre Questions, 'count': 144}, {'val': Questions a…  │       493 │\n",
      "│ Parenting            │ [{'val': Interpreting Parental Relationships, 'count': 76}, {'val': Answering Q…  │       478 │\n",
      "│ Film Country Ident…  │ [{'val': Filmmaker Nationality Questions, 'count': 112}, {'val': Nationality Co…  │       429 │\n",
      "│ Cloning              │ [{'val': Understanding Cloning: Questions and Answers, 'count': 65}, {'val': Co…  │       427 │\n",
      "│ Dining Etiquette     │ [{'val': Answering Formal Dining Etiquette, 'count': 48}, {'val': American Dini…  │       425 │\n",
      "│ Inception Relations  │ [{'val': Inception Relation Questions for Portland Streetcar, 'count': 49}, {'v…  │       386 │\n",
      "│ Water Identification │ [{'val': Identifying Water Bodies in Texts, 'count': 57}, {'val': Interpreting …  │       385 │\n",
      "│ Railway Station Re…  │ [{'val': British Railways and Regional Information, 'count': 61}, {'val': Railw…  │       363 │\n",
      "│ Elephant Studies     │ [{'val': Elephant Orphanage Articles, 'count': 70}, {'val': Understanding Eleph…  │       352 │\n",
      "├──────────────────────┴───────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
      "│ 666 rows (20 shown)                                                                                        3 columns │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "sql took 0.175s.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# /Users/dsmilkov/code/lilac/data/datasets/local/openorca/question__cluster-00000-of-00001.parquet\n",
    "# /Users/dsmilkov/code/lilac/data/datasets/local/SlimOrca-10k-sample/prompt__cluster-00000-of-00001.parquet\n",
    "con = duckdb.connect()\n",
    "con.sql(\n",
    "  'CREATE VIEW t AS (SELECT * from read_parquet(\"/Users/dsmilkov/code/lilac/data/datasets/local/openorca/question__cluster-00000-of-00001.parquet\"))'\n",
    ")\n",
    "\n",
    "from lilac.utils import DebugTimer\n",
    "\n",
    "with DebugTimer('sql'):\n",
    "  res = con.sql(\n",
    "    \"\"\"\n",
    "    SELECT category, list({'val': val, 'count': count} ORDER BY count DESC) as vals, sum(count) as cat_count from\n",
    "    (\n",
    "      SELECT category, val, count() as count from (\n",
    "        SELECT question__cluster.cluster_title as val,\n",
    "              question__cluster.category_title as category from t\n",
    "      )\n",
    "      GROUP BY category, val\n",
    "    )\n",
    "    GROUP BY category, ORDER BY cat_count DESC\n",
    "  \"\"\"\n",
    "  )\n",
    "  print(repr(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
