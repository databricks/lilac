{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a LlamaIndex loader with Lilac\n",
    "\n",
    "This notebook will show you how to load any LlamaIndex loader and load the data into Lilac.\n",
    "\n",
    "LlamaIndex loaders [can be found on LlamaHub](https://llamahub.ai/).\n",
    "\n",
    "In this example, we'll use the [ArxivReader loader from LlamaHub](https://llamahub.ai/l/papers-arxiv), and load arxiv papers into Lilac.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.9/site-packages (3.16.2)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from pypdf) (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "# See: https://llamahub.ai/l/papers-arxiv\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "loader = ArxivReader()\n",
    "documents = loader.load_data(search_query='au:Karpathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil/Code/lilac/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lilac as ll\n",
    "\n",
    "# Set the project directory for Lilac.\n",
    "ll.set_project_dir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from source llama_index_docs...: 100%|██████████| 107/107 [00:00<00:00, 10133.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing:\n",
      "SELECT COUNT() as count FROM t\n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"doc_id\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"text\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"page_label\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"file_name\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"Title of this paper\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"Authors\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"Date published\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"doc_id\" as val FROM t)\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"file_name\" as val FROM t)\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"Title of this paper\" as val FROM t)\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"page_label\" as val FROM t)\n",
      "Executing:\n",
      "\n",
      "        SELECT avg(length(val))\n",
      "        FROM (SELECT \"URL\" AS val FROM t) USING SAMPLE 1000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"Date published\" as val FROM t)\n",
      "Query took 0.002s.\n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"Authors\" as val FROM t)\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"text\" as val FROM t)\n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"doc_id\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"file_name\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"Title of this paper\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"page_label\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "SELECT count(val) FROM (SELECT \"URL\" as val FROM t)\n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"Date published\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"Authors\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Query took 0.001s.\n",
      "Executing:\n",
      "\n",
      "        SELECT approx_count_distinct(val) as approxCountDistinct\n",
      "        FROM (SELECT \"URL\" AS val FROM t) USING SAMPLE 500000;\n",
      "      \n",
      "Query took 0.000s.\n",
      "Dataset \"arxiv-karpathy\" written to ./data/datasets/local/arxiv-karpathy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lilac.data.dataset_duckdb.DatasetDuckDB at 0x2a843ff10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This assumes you already have a lilac project set up.\n",
    "# If you don't, use ll.init(project_dir='./data')\n",
    "ll.create_dataset(\n",
    "  config=ll.DatasetConfig(\n",
    "    namespace='local',\n",
    "    name='arxiv-karpathy',\n",
    "    source=ll.LlamaIndexDocsSource(\n",
    "      # documents comes from the loader.load_data call in the previous cell.\n",
    "      documents=documents,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing:\n",
      "SELECT COUNT() as count FROM t\n",
      "Query took 0.000s.\n",
      "{'doc_id': 'd882b74b-1c27-4f44-aa5d-4e25683b9f5a', 'text': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning\\nJustin Johnson∗Andrej Karpathy∗Li Fei-Fei\\nDepartment of Computer Science, Stanford University\\n{jcjohns, karpathy, feifeili }@cs.stanford.edu\\nAbstract\\nWe introduce the dense captioning task, which requires a\\ncomputer vision system to both localize and describe salient\\nregions in images in natural language. The dense caption-\\ning task generalizes object detection when the descriptions\\nconsist of a single word, and Image Captioning when one\\npredicted region covers the full image. To address the local-\\nization and description task jointly we propose a Fully Con-\\nvolutional Localization Network (FCLN) architecture that\\nprocesses an image with a single, efﬁcient forward pass, re-\\nquires no external regions proposals, and can be trained\\nend-to-end with a single round of optimization. The archi-\\ntecture is composed of a Convolutional Network, a novel\\ndense localization layer, and Recurrent Neural Network\\nlanguage model that generates the label sequences. We\\nevaluate our network on the Visual Genome dataset, which\\ncomprises 94,000 images and 4,100,000 region-grounded\\ncaptions. We observe both speed and accuracy improve-\\nments over baselines based on current state of the art ap-\\nproaches in both generation and retrieval settings.\\n1. Introduction\\nOur ability to effortlessly point out and describe all aspects\\nof an image relies on a strong semantic understanding of a\\nvisual scene and all of its elements. However, despite nu-\\nmerous potential applications, this ability remains a chal-\\nlenge for our state of the art visual recognition systems.\\nIn the last few years there has been signiﬁcant progress\\nin image classiﬁcation [38, 26, 52, 44], where the task is\\nto assign one label to an image. Further work has pushed\\nthese advances along two orthogonal directions: First, rapid\\nprogress in object detection [39, 15, 45] has identiﬁed mod-\\nels that efﬁciently identify and label multiple salient regions\\nof an image. Second, recent advances in image captioning\\n[4, 32, 22, 48, 50, 9, 5] have expanded the complexity of\\nthe label space from a ﬁxed set of categories to sequence of\\nwords able to express signiﬁcantly richer concepts.\\nHowever, despite encouraging progress along the label\\ndensity and label complexity axes, these two directions have\\n∗Indicates equal contribution.\\nClassification \\nCat\\nCaptioning \\nA cat \\nriding a \\nskateboard \\nDetection \\nCat\\nSkateboard \\nDense Captioning \\nOrange spotted cat \\nSkateboard with \\nred wheels \\nCat riding a \\nskateboard \\nBrown hardwood \\nflooring label density Whole Image Image Regions \\nlabel \\ncomplexity Single \\nLabel \\nSequence Figure 1. We address the Dense Captioning task (bottom right) by\\ngenerating dense, rich annotations with a single forward pass.\\nremained separate. In this work we take a step towards uni-\\nfying these two inter-connected tasks into one joint frame-\\nwork. First, we introduce the dense captioning task (see\\nFigure 1), which requires a model to predict a set of descrip-\\ntions across regions of an image. Object detection is hence\\nrecovered as a special case when the target labels consist\\nof one word, and image captioning is recovered when all\\nimages consist of one region that spans the full image.\\nAdditionally, we develop a Fully Convolutional Local-\\nization Network architecture (FCLN) to address the dense\\ncaptioning task. Our model is inspired by recent work in\\nimage captioning [48, 22, 32, 9, 5] in that it is composed\\nof a Convolutional Neural Network followed by a Recur-\\nrent Neural Network language model. However, drawing on\\nwork in object detection [37], our second core contribution\\nis to introduce a new dense localization layer. This layer is\\nfully differentiable and can be inserted into any neural net-\\nwork that processes images to enable region-level training\\nand predictions. Internally, the localization layer predicts a\\nset of regions of interest in the image and then uses bilin-\\near interpolation [20, 17] to smoothly extract the activations\\ninside each region.\\nWe evaluate the model on the large-scale Visual Genome\\ndataset, which contains 94,000 images and 4,100,000 region\\ncaptions. Our results show both performance and speed im-\\nprovements over approaches based on previous state of the\\nart. We make our code and data publicly available to sup-\\nport further progress on the dense captioning task.\\n1arXiv:1511.07571v1  [cs.CV]  24 Nov 2015', 'page_label': '1', 'file_name': '378e92ec30c36727ce7589912ae2e1e0.pdf', 'Title of this paper': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning', 'Authors': 'Justin Johnson, Andrej Karpathy, Li Fei-Fei', 'Date published': '11/24/2015', 'URL': 'http://arxiv.org/abs/1511.07571v1'}\n",
      "{'doc_id': 'a6ddfdea-f54b-43ca-ad01-decbcf832288', 'text': '2. Related Work\\nOur work draws on recent work in object detection, im-\\nage captioning, and soft spatial attention that allows down-\\nstream processing of particular regions in the image.\\nObject Detection. Our core visual processing module is a\\nConvolutional Neural Network (CNN) [29, 26], which has\\nemerged as a powerful model for visual recognition tasks\\n[38]. The ﬁrst application of these models to dense predic-\\ntion tasks was introduced in R-CNN [15], where each re-\\ngion of interest was processed independently. Further work\\nhas focused on processing all regions with only single for-\\nward pass of the CNN [18, 14], and on eliminating explicit\\nregion proposal methods by directly predicting the bound-\\ning boxes either in the image coordinate system [45, 10],\\nor in a fully convolutional [31] and hence position-invariant\\nsettings [39, 37, 36]. Most related to our approach is the\\nwork of Ren et al. [37] who develop a region proposal net-\\nwork (RPN) that regresses from anchors to regions of in-\\nterest. However, they adopt a 4-step optimization process,\\nwhile our approach does not require training pipelines. Ad-\\nditionally, we replace their RoI pooling mechanism with a\\ndifferentiable, spatial soft attention mechanism [20, 17]. In\\nparticular, this change allows us to backpropagate through\\nthe region proposal network and train the whole model\\njointly.\\nImage Captioning. Several pioneering approaches have\\nexplored the task of describing images with natural lan-\\nguage [1, 27, 13, 34, 41, 42, 28, 21]. More recent ap-\\nproaches based on neural networks have adopted Recurrent\\nNeural Networks (RNNs) [49, 19] as the core architectural\\nelement for generating captions. These models have pre-\\nviously been used in language modeling [2, 16, 33, 43],\\nwhere they are known to learn powerful long-term inter-\\nactions [23]. Several recent approaches to Image Caption-\\ning [32, 22, 48, 9, 5, 25, 12] rely on a combination of RNN\\nlanguage model conditioned on image information. A re-\\ncent related approach is the work of Xu et al. [50] who use\\na soft attention mechanism [6] over regions of the input im-\\nage with every generated word. Our approach to spatial at-\\ntention is more general in that the network can process ar-\\nbitrary afﬁne regions in the image instead of only discrete\\ngrid positions in an intermediate conv volume. However, for\\nsimplicity, during generation we follow Vinyals et al. [48],\\nwhere the visual information is only passed to the language\\nmodel once on the ﬁrst time step.\\nFinally, the metrics we develop for the dense captioning\\ntask are inspired by metrics developed for image captioning\\n[47, 8, 4].\\n3. Model\\nOverview. Our goal is to design an architecture that jointly\\nlocalizes regions of interest and then describes each with\\nnatural language. The primary challenge is to develop amodel that supports end-to-end training with a single step of\\noptimization, and both efﬁcient and effective inference. Our\\nproposed architecture (see Figure 2) draws on architectural\\nelements present in recent work on object detection, image\\ncaptioning and soft spatial attention to simultaneously ad-\\ndress these design constraints.\\nIn Section 3.1 we ﬁrst describe the components of our\\nmodel. Then in Sections 3.2 and 3.3 we address the loss\\nfunction and the details of training and inference.\\n3.1. Model Architecture\\n3.1.1 Convolutional Network\\nWe use the VGG-16 architecture [40] for its state-of-the-art\\nperformance [38]. It consists of 13 layers of 3×3con-\\nvolutions interspersed with 5 layers of 2×2max pooling.\\nWe remove the ﬁnal pooling layer, so an input image of\\nshape 3×W×Hgives rise to a tensor of features of shape\\nC×W′×H′whereC= 512 ,W′=⌊W\\n16⌋\\n, andH′=⌊H\\n16⌋\\n.\\nThe output of this network encodes the appearance of the\\nimage at a set of uniformly sampled image locations, and\\nforms the input to the localization layer.\\n3.1.2 Fully Convolutional Localization Layer\\nThe localization layer receives an input tensor of activa-\\ntions, identiﬁes spatial regions of interest and smoothly ex-\\ntracts a ﬁxed-sized representation from each region. Our\\napproach is based on that of Faster R-CNN [37], but we\\nreplace their RoI pooling mechanism [14] with bilinear\\ninterpolation [20], allowing our model to propagate gra-\\ndients backward through the coordinates of predicted re-\\ngions. This modiﬁcation opens up the possibility of predict-\\ning afﬁne or morphed region proposals instead of bounding\\nboxes [20], but we leave these extensions to future work.\\nInputs/outputs . The localization layer accepts a tensor of\\nactivations of size C×W′×H′. It then internally selects\\nBregions of interest and returns three output tensors giving\\ninformation about these regions:\\n1.Region Coordinates : A matrix of shape B×4giving\\nbounding box coordinates for each output region.\\n2.Region Scores : A vector of length Bgiving a con-\\nﬁdence score for each output region. Regions with\\nhigh conﬁdence scores are more likely to correspond\\nto ground-truth regions of interest.\\n3.Region Features : A tensor of shape B×C×X×Y\\ngiving features for output regions; is represented by an\\nX×Ygrid ofC-dimensional features.\\nConvolutional Anchors . Similar to Faster R-CNN [37],\\nour localization layer predicts region proposals by regress-\\ning offsets from a set of translation-invariant anchors. In\\nparticular, we project each point in the W′×H′grid of', 'page_label': '2', 'file_name': '378e92ec30c36727ce7589912ae2e1e0.pdf', 'Title of this paper': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning', 'Authors': 'Justin Johnson, Andrej Karpathy, Li Fei-Fei', 'Date published': '11/24/2015', 'URL': 'http://arxiv.org/abs/1511.07571v1'}\n",
      "{'doc_id': '28ad9c16-0a09-4a56-b2bf-403c61f25768', 'text': 'CNN Image: \\n3 x W x H Conv features:  \\nC x W’ x H’ Region features: \\nB x C x X x Y Region Codes: \\nB x D \\nLSTM \\nStriped gray cat \\nCats watching TV \\nLocalization Layer \\nConv Region Proposals: \\n4k x W’ x H’ \\nRegion scores: \\nk x W’ x H’ Conv features: \\nC x W’ x H’ Bilinear Sampler Region features: \\nB x 512 x 7 x 7 Sampling Grid: \\nB x X x Y x 2 \\nSampling Grid \\nGenerator Best Proposals: \\nB x 4 Recognition \\nNetwork \\nFigure 2. Model overview. An input image is ﬁrst processed a CNN. The Localization Layer proposes regions and smoothly extracts a\\nbatch of corresponding activations using bilinear interpolation. These regions are processed with a fully-connected recognition network\\nand described with an RNN language model. The model is trained end-to-end with gradient descent.\\ninput features back into the W×Himage plane, and con-\\nsiderkanchor boxes of different aspect ratios centered at\\nthis projected point. For each of these kanchor boxes,\\nthe localization layer predicts a conﬁdence score and four\\nscalars regressing from the anchor to the predicted box co-\\nordinates. These are computed by passing the input feature\\nmap through a 3×3convolution with 256 ﬁlters, a rectiﬁed\\nlinear nonlinearity, and a 1×1convolution with 5kﬁlters.\\nThis results in a tensor of shape 5k×W′×H′containing\\nscores and offsets for all anchors.\\nBox Regression . We adopt the parameterization of [14]\\nto regress from anchors to the region proposals. Given an\\nanchor box with center (xa,ya), widthwa, and height ha,\\nour model predicts scalars (tx,ty,tw,th)giving normalized\\noffsets and log-space scaling transforms, so that the output\\nregion has center (x,y)and shape (w,h)given by\\nx=xa+txway=ya+tyha (1)\\nw=waexp(tw)h=haexp(hw) (2)\\nBox Sampling . Processing a typical image of size W=\\n720,H= 540 withk= 12 anchor boxes gives rise to\\n17,280 region proposals. Since running the recognition net-\\nwork and the language model for all proposals would be\\nprohibitively expensive, it is necessary to subsample them.\\nAt training time, we follow the approach of [37] and\\nsample a minibatch containing B= 256 boxes with at most\\nB/2positive regions and the rest negatives. A region is pos-\\nitive if it has an intersection over union (IoU) of at least 0.7\\nwith some ground-truth region; in addition, the predicted\\nregion of maximal IoU with each ground-truth region is\\npositive. A region is negative if it has IoU <0.3with\\nall ground-truth regions. Our sampled minibatch containsBP≤B/2positive regions and BN=B−BPnegative\\nregions, sampled uniformly without replacement from the\\nset of all positive and all negative regions respectively.\\nAt test time we subsample using greedy non-maximum\\nsuppression (NMS) based on the predicted proposal conﬁ-\\ndences to select the B= 300 most conﬁdent propoals.\\nThe coordinates and conﬁdences of the sampled propos-\\nals are collected into tensors of shape B×4andBrespec-\\ntively, and are output from the localization layer.\\nBilinear Interpolation. After sampling, we are left with\\nregion proposals of varying sizes and aspect ratios. In order\\nto interface with the full-connected recognition network and\\nthe RNN language model, we must extract a ﬁxed-size fea-\\nture representation for each variably sized region proposal.\\nTo solve this problem, Fast R-CNN [14] proposes an RoI\\npooling layer where each region proposal is projected onto\\ntheW′×H′grid of convolutional features and divided into\\na coarseX×Ygrid aligned to pixel boundaries by round-\\ning. Features are max-pooled within each grid cell, result-\\ning in anX×Ygrid of output features.\\nThe RoI pooling layer is a function of two inputs: convo-\\nlutional features and region proposal coordinates. Gradients\\ncan be propagated backward from the output features to the\\ninput features, but not to the input proposal coordinates. To\\novercome this limitation, we replace the RoI pooling layer\\nwith with bilinear interpolation [17, 20].\\nConcretely, given an input feature map Uof shapeC×\\nW′×H′and a region proposal, we interpolate the features\\nofUto produce an output feature map Vof shapeC×X×\\nY. After projecting the region proposal onto Uwe follow\\n[20] and compute a sampling grid Gof shapeX×Y×2\\nassociating each element of Vwith real-valued coordinates', 'page_label': '3', 'file_name': '378e92ec30c36727ce7589912ae2e1e0.pdf', 'Title of this paper': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning', 'Authors': 'Justin Johnson, Andrej Karpathy, Li Fei-Fei', 'Date published': '11/24/2015', 'URL': 'http://arxiv.org/abs/1511.07571v1'}\n",
      "{'doc_id': '91f377f2-e69e-4289-bf6a-0a651ffb308d', 'text': 'intoU. IfGi,j= (xi,j,yi,j)thenVc,i,jshould be equal to U\\nat(c,xi,j,yi,j); however since (xi,j,yi,j)are real-valued,\\nwe convolve with a sampling kernel kand set\\nVc,i,j=W∑\\ni′=1H∑\\nj′=1Uc,i′,j′k(i′−xi,j)k(j′−yi,j).(3)\\nWe use bilinear sampling, corresponding to the kernel\\nk(d) = max(0 ,1−|d|). The sampling grid is a linear\\nfunction of the proposal coordinates, so gradients can be\\npropagated backward into predicted region proposal coordi-\\nnates. Running bilinear interpolation to extract features for\\nall sampled regions gives a tensor of shape B×C×X×Y,\\nforming the ﬁnal output from the localization layer.\\n3.1.3 Recognition Network\\nThe recognition network is a fully-connected neural net-\\nwork that processes region features from the localization\\nlayer. The features from each region are ﬂattened into a vec-\\ntor and passed through two full-connected layers, each us-\\ning rectiﬁed linear units and regularized using Dropout. For\\neach region this produces a code of dimension D= 4096\\nthat compactly encodes its visual appearance. The codes\\nfor all positive regions are collected into a matrix of shape\\nB×Dand passed to the RNN language model.\\nIn addition, we allow the recognition network one more\\nchance to reﬁne the conﬁdence and position of each pro-\\nposal region. It outputs a ﬁnal scalar conﬁdence of each pro-\\nposed region and four scalars encoding a ﬁnal spatial off-\\nset to be applied to the region proposal. These two outputs\\nare computed as a linear transform from the D-dimensional\\ncode for each region. The ﬁnal box regression uses the same\\nparameterization as Section 3.1.2.\\n3.1.4 RNN Language Model\\nFollowing previous work [32, 22, 48, 9, 5], we use the\\nregion codes to condition an RNN language model [16,\\n33, 43]. Concretely, given a training sequence of to-\\nkenss1,...,s T, we feed the RNN T+ 2 word vectors\\nx−1,x0,x1,...,x T, wherex−1=CNN (I)is the region\\ncode encoded with a linear layer and followed by a ReLU\\nnon-linearity, x0corresponds to a special START token, and\\nxtencode each of the tokens st,t= 1,...,T . The RNN\\ncomputes a sequence of hidden states htand output vectors\\nytusing a recurrence formula ht,yt=f(ht−1,xt)(we use\\nthe LSTM [19] recurrence). The vectors ythave size|V|+1\\nwhereVis the token vocabulary, and where the additional\\none is for a special END token. The loss function on the\\nvectorsytis the average cross entropy, where the targets at\\ntimest= 0,...,T−1are the token indices for st+1, and\\nthe target at t=Tis the END token. The vector y−1is\\nignored. Our tokens and hidden layers have size 512.\\nAt test time we feed the visual information x−1to the\\nRNN. At each time step we sample the most likely nexttoken and feed it to the RNN in the next time step, repeating\\nthe process until the special END token is sampled.\\n3.2. Loss function\\nDuring training our ground truth consists of positive boxes\\nand descriptions. Our model predicts positions and conﬁ-\\ndences of sampled regions twice: in the localization layer\\nand again in the recognition network. We use binary logis-\\ntic lossses for the conﬁdences trained on sampled positive\\nand negative regions. For box regression, we use a smooth\\nL1 loss in transform coordinate space similar to [37]. The\\nﬁfth term in our loss function is a cross-entropy term at ev-\\nery time-step of the language model.\\nWe normalize all loss functions by the batch size and\\nsequence length in the RNN. We searched over an effec-\\ntive setting of the weights between these contributions and\\nfound that a reasonable setting is to use a weight of 0.1 for\\nthe ﬁrst four criterions, and a weight of 1.0 for captioning.\\n3.3. Training and optimization\\nWe train the full model end-to-end in a single step of opti-\\nmization. We initialize the CNN with weights pretrained on\\nImageNet [38] and all other weights from a gaussian with\\nstandard deviation of 0.01. We use stochastic gradient de-\\nscent with momentum 0.9 to train the weights of the con-\\nvolutional network, and Adam [24] to train the other com-\\nponents of the model. We use a learning rate of 1×10−6\\nand setβ1= 0.9,β2= 0.99. We begin ﬁne-tuning the lay-\\ners of the CNN after 1 epoch, and for efﬁciency we do not\\nﬁne-tune the ﬁrst four convolutional layers of the network.\\nOur training batches consist of a single image that has\\nbeen resized so that the longer side has 720 pixels. Our\\nimplementation uses Torch7 [7] and [35]. One mini-batch\\nruns in approximately 300ms on a Titan X GPU and it takes\\nabout three days of training for the model to converge.\\n4. Experiments\\nDataset . Existing datasets that relate images and natural\\nlanguage either only include full image captions [4, 51],\\nor ground words of image captions in regions but do not\\nprovide individual region captions [3]. We perform our ex-\\nperiments using the Visual Genome (VG) region captions\\ndataset1This dataset contains 94,313 images and 4,100,413\\nsnippets of text (43.5 per image), each grounded to a re-\\ngion of an image. Images were taken from the intersection\\nof MS COCO and YFCC100M [46], and annotations were\\ncollected on Amazon Mechanical Turk by asking workers\\nto draw a bounding box on the image and describe its con-\\ntent in text. Example captions from the dataset include “cats\\nplay with toys hanging from a perch”, “newspapers are scat-\\ntered across a table”, “woman pouring wine into a glass”,\\n“mane of a zebra”, and “red light”.\\n1Dataset in submission, obtained via personal communication. We\\ncommit to releasing the relevant parts upon publication.', 'page_label': '4', 'file_name': '378e92ec30c36727ce7589912ae2e1e0.pdf', 'Title of this paper': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning', 'Authors': 'Justin Johnson, Andrej Karpathy, Li Fei-Fei', 'Date published': '11/24/2015', 'URL': 'http://arxiv.org/abs/1511.07571v1'}\n",
      "{'doc_id': '931d8fba-b5b0-4960-941f-cb0debb4c01e', 'text': 'A man and a woman sitting \\nat a table with a cake. A train is traveling down the tracks near a forest. A large jetliner flying through a blue sky. A teddy bear with \\na red bow on it. Our Model: \\nFull Image RNN: Figure 3. Example captions generated and localized by our model on test images. We render the top few most conﬁdent predictions. On\\nthe bottom row we additionally contrast the amount of information our model generates compared to the Full image RNN.\\nPreprocessing . We collapse words that appear less than\\n15 times into a special <UNK> token, giving a vocabulary\\nof 10,497 words. We strip referring phrases such as “there\\nis...”, or “this seems to be a”. For efﬁciency we discard all\\nannotations with more than 10 words (7% of annotations).\\nWe also discard all images that have fewer than 20 or more\\nthan 50 annotations to reduce the variation in the number\\nof regions per image. We are left with 87,398 images; we\\nassign 5,000 each to val/test splits and the rest to train.\\nFor test time evaluation we also preprocess the ground\\ntruth regions in the validation/test images by merging heav-\\nily overlapping boxes into single boxes with several refer-\\nence captions. For each image we iteratively select the box\\nwith the highest number of overlapping boxes (based on\\nIoU with threshold of 0.7), and merge these together (by\\ntaking the mean) into a single box with multiple reference\\ncaptions. We then exclude this group and repeat the process.\\n4.1. Dense Captioning\\nIn the dense captioning task the model receives a single im-\\nage and produces a set of regions, each annotated with a\\nconﬁdence and a caption.\\nEvaluation metrics . Intuitively, we would like our model\\nto produce both well-localized predictions (as in object de-\\ntection) and accurate descriptions (as in image captioning).\\nInspired by evaluation metrics in object detection [11,30] and image captioning [47], we propose to measure the\\nmean Average Precision (AP) across a range of thresholds\\nfor both localization and language accuracy. For localiza-\\ntion we use intersection over union (IoU) thresholds .3, .4,\\n.5, .6, .7. For language we use METEOR score thresholds\\n0, .05, .1, .15, .2, .25. We adopt METEOR since this metric\\nwas found to be most highly correlated with human judg-\\nments in settings with a low number of references [47]. We\\nmeasure the average precision across all pairwise settings\\nof these thresholds and report the mean AP.\\nTo isolate the accuracy of language in the predicted cap-\\ntions without localization we also merge ground truth cap-\\ntions across each test image into a bag of references sen-\\ntences and evaluate predicted captions with respect to these\\nreferences without taking into account their spatial position.\\nBaseline models . Following Karpathy and Fei-Fei [22], we\\ntrain only the Image Captioning model (excluding the local-\\nization layer) on individual, resized regions. We refer to this\\napproach as a Region RNN model . To investigate the differ-\\nences between captioning trained on full images or regions\\nwe also train the same model on full images and captions\\nfrom MS COCO ( Full Image RNN model ).\\nAt test time we consider three sources of region propos-\\nals. First, to establish an upper bound we evaluate the model\\non ground truth boxes (GT). Second, similar to [22] we use', 'page_label': '5', 'file_name': '378e92ec30c36727ce7589912ae2e1e0.pdf', 'Title of this paper': 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning', 'Authors': 'Justin Johnson, Andrej Karpathy, Li Fei-Fei', 'Date published': '11/24/2015', 'URL': 'http://arxiv.org/abs/1511.07571v1'}\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows:\n",
    "dataset = ll.get_dataset('local', 'arxiv-karpathy')\n",
    "for row in dataset.select_rows(['*'], limit=5):\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2276]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:5432 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# You can start a lilac server with:\n",
    "ll.start_server(project_dir='./data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
